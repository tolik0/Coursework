{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../object_detection')\n",
    "from image_detector import crop_minAreaRect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Apple Braeburn',\n",
    " 'Apple Golden 1',\n",
    " 'Apple Golden 2',\n",
    " 'Apple Golden 3',\n",
    " 'Apple Granny Smith',\n",
    " 'Apple Red 1',\n",
    " 'Apple Red 2',\n",
    " 'Apple Red 3',\n",
    " 'Apple Red Delicious',\n",
    " 'Apple Red Yellow',\n",
    " 'Apricot',\n",
    " 'Avocado',\n",
    " 'Avocado ripe',\n",
    " 'Banana',\n",
    " 'Banana Red',\n",
    " 'Cactus fruit',\n",
    " 'Carambula',\n",
    " 'Cherry',\n",
    " 'Clementine',\n",
    " 'Cocos',\n",
    " 'Dates',\n",
    " 'Garbage',\n",
    " 'Granadilla',\n",
    " 'Grape Pink',\n",
    " 'Grape White',\n",
    " 'Grape White 2',\n",
    " 'Grapefruit Pink',\n",
    " 'Grapefruit White',\n",
    " 'Guava',\n",
    " 'Huckleberry',\n",
    " 'Kaki',\n",
    " 'Kiwi',\n",
    " 'Kumquats',\n",
    " 'Lemon',\n",
    " 'Lemon Meyer',\n",
    " 'Limes',\n",
    " 'Litchi',\n",
    " 'Mandarine',\n",
    " 'Mango',\n",
    " 'Maracuja',\n",
    " 'Nectarine',\n",
    " 'Orange',\n",
    " 'Papaya',\n",
    " 'Passion Fruit',\n",
    " 'Peach',\n",
    " 'Peach Flat',\n",
    " 'Pear',\n",
    " 'Pear Abate',\n",
    " 'Pear Monster',\n",
    " 'Pear Williams',\n",
    " 'Pepino',\n",
    " 'Pineapple',\n",
    " 'Pitahaya Red',\n",
    " 'Plum',\n",
    " 'Pomegranate',\n",
    " 'Quince',\n",
    " 'Raspberry',\n",
    " 'Salak',\n",
    " 'Strawberry',\n",
    " 'Tamarillo',\n",
    " 'Tangelo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "def image_loader(image):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = loader(image).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image.cuda()  #assumes that you're using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"model_finetuning_v2.pth\"\n",
    "image_path = \"../test_images/ban_app.jpg\"\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects(path):\n",
    "    boundaries = [(np.array([10, 10, 10]), np.array([28, 255, 255])), #yellow\n",
    "                  (np.array([28, 10, 10]), np.array([80, 255, 255])), #green\n",
    "                  (np.array([0, 10, 10]), np.array([10, 255, 255]))] #red\n",
    "\n",
    "\n",
    "    image = cv2.imread(path)\n",
    "    bordersize = 50\n",
    "    image = cv2.copyMakeBorder(image, top=bordersize, bottom=bordersize,\n",
    "                                left=bordersize, right=bordersize,\n",
    "                                borderType=cv2.BORDER_CONSTANT,\n",
    "                                value=[255, 255, 255])\n",
    "    image1 = image.copy()\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    images = []\n",
    "\n",
    "    for (lower, upper) in boundaries:\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "        res = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        im2, contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE,\n",
    "                                                    cv2.CHAIN_APPROX_SIMPLE)\n",
    "        index = []\n",
    "\n",
    "        for i in range(len(contours)):\n",
    "            if len(contours[i]) < 50:\n",
    "                index.append(i)\n",
    "\n",
    "        contours = np.delete(contours, index)\n",
    "\n",
    "        for contour in contours:\n",
    "            rect = cv2.minAreaRect(contour)\n",
    "\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            startX, startY = max(box, key = lambda x: x[0])[0], min(box, key = lambda x: x[1])[1]\n",
    "            cv2.drawContours(image1, [box], 0, (0, 0, 255), 2)\n",
    "            \n",
    "            # display the prediction\n",
    "            img = crop_minAreaRect(image, rect)\n",
    "            croped_image = image_loader(Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "            outputs = model(croped_image)[0]\n",
    "            outputs = (outputs.data).cpu().numpy()\n",
    "            ind = outputs.argsort()[-5:][::-1]\n",
    "            \n",
    "            label = \"\"\n",
    "            for i in ind:\n",
    "                label += \"  {}: {}\".format(names[i], outputs[i]) + \"\\n\"\n",
    "            print(label)\n",
    "        \n",
    "            y, dy = startY, 15\n",
    "            for line in label.split('\\n'):\n",
    "                y += dy\n",
    "                cv2.putText(image1, line, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1)\n",
    "\n",
    "            img[np.where((img == [0, 0, 0]).all(axis=2))] = [255, 255, 255]\n",
    "            images.append(img)\n",
    "            \n",
    "    cv2.imshow('image', image1)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 57 206]\n",
      " [ 57  50]\n",
      " [209  50]\n",
      " [209 206]]\n",
      "  Garbage: 12.43777847290039\n",
      "  Grape White 2: 8.503541946411133\n",
      "  Quince: 7.701147079467773\n",
      "  Apple Golden 2: 7.6313982009887695\n",
      "  Maracuja: 6.345088005065918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images = find_objects(\"../test_images/3.jpg\")\n",
    "\n",
    "# for i in range(len(images)):\n",
    "#     cv2.imshow(f\"{i}\", images[i])\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
