{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../object_detection')\n",
    "from image_detector import crop_minAreaRect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['acerolas',\n",
    " 'apples',\n",
    " 'apricots',\n",
    " 'avocados',\n",
    " 'bananas',\n",
    " 'blackberries',\n",
    " 'blueberries',\n",
    " 'cantaloupes',\n",
    " 'cherries',\n",
    " 'coconuts',\n",
    " 'figs',\n",
    " 'garbage',\n",
    " 'grapefruits',\n",
    " 'grapes',\n",
    " 'guava',\n",
    " 'kiwifruit',\n",
    " 'lemons',\n",
    " 'limes',\n",
    " 'mangos',\n",
    " 'olives',\n",
    " 'oranges',\n",
    " 'passionfruit',\n",
    " 'peaches',\n",
    " 'pears',\n",
    " 'pineapples',\n",
    " 'plums',\n",
    " 'pomegranates',\n",
    " 'raspberries',\n",
    " 'strawberries',\n",
    " 'tomatoes',\n",
    " 'watermelons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "def image_loader(image):\n",
    "    \"\"\"load image, returns cuda tensor\"\"\"\n",
    "    image = loader(image).float()\n",
    "    image = Variable(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)  #this is for VGG, may not be needed for ResNet\n",
    "    return image.cuda()  #assumes that you're using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mymodel_finetuning_new_v1.pth\"\n",
    "image_path = \"../test_images/ban_app.jpg\"\n",
    "model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects(path):\n",
    "    boundaries = [(np.array([10, 10, 10]), np.array([28, 255, 255])), #yellow\n",
    "                  (np.array([28, 10, 10]), np.array([80, 255, 255])), #green\n",
    "                  (np.array([0, 10, 10]), np.array([10, 255, 255]))] #red\n",
    "\n",
    "\n",
    "    image = cv2.imread(path)\n",
    "    bordersize = 50\n",
    "    image = cv2.copyMakeBorder(image, top=bordersize, bottom=bordersize,\n",
    "                                left=bordersize, right=bordersize,\n",
    "                                borderType=cv2.BORDER_CONSTANT,\n",
    "                                value=[255, 255, 255])\n",
    "    image1 = image.copy()\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    images = []\n",
    "\n",
    "    for (lower, upper) in boundaries:\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "        res = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        im2, contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE,\n",
    "                                                    cv2.CHAIN_APPROX_SIMPLE)\n",
    "        index = []\n",
    "\n",
    "        for i in range(len(contours)):\n",
    "            if len(contours[i]) < 50:\n",
    "                index.append(i)\n",
    "\n",
    "        contours = np.delete(contours, index)\n",
    "\n",
    "        for contour in contours:\n",
    "            rect = cv2.minAreaRect(contour)\n",
    "\n",
    "            box = cv2.boxPoints(rect)\n",
    "            box = np.int0(box)\n",
    "            startX, startY = min(box, key = lambda x: x[0])[0], min(box, key = lambda x: x[1])[1]\n",
    "            cv2.drawContours(image1, [box], 0, (0, 0, 255), 2)\n",
    "            \n",
    "            # display the prediction\n",
    "            img = crop_minAreaRect(image, rect)\n",
    "            croped_image = image_loader(Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)))\n",
    "            outputs = torch.nn.Softmax()(model(croped_image)).data.cpu().numpy()[0]\n",
    "            print(outputs)\n",
    "            ind = outputs.argsort()[-3:][::-1]\n",
    "            \n",
    "            label = \"\"\n",
    "            for i in ind:\n",
    "                label += \"  {}: {}%\".format(names[i], round(outputs[i]*100,1)) + \"\\n\"\n",
    "            print(label)\n",
    "        \n",
    "            y, dy = startY, 15\n",
    "            for line in label.split('\\n'):\n",
    "                y += dy\n",
    "                cv2.putText(image1, line, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
    "\n",
    "            img[np.where((img == [0, 0, 0]).all(axis=2))] = [255, 255, 255]\n",
    "            images.append(img)\n",
    "            \n",
    "    cv2.imshow('image', image1)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    return image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iatsuk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.59186165e-04 4.50112343e-01 3.26808513e-05 3.36645142e-04\n",
      " 5.71921095e-03 2.49520435e-05 7.94728840e-06 6.45525142e-05\n",
      " 1.13572831e-04 1.24890925e-04 9.11979377e-03 1.27475214e-04\n",
      " 2.54393017e-05 1.25048484e-03 4.92807776e-02 1.53959764e-03\n",
      " 1.00498600e-02 3.74610275e-02 7.15936348e-02 7.09260255e-03\n",
      " 2.09497412e-05 3.89611414e-05 1.15620176e-04 3.50741476e-01\n",
      " 1.88407823e-04 2.24325020e-04 6.10781135e-05 4.04313141e-05\n",
      " 9.66388252e-05 1.23582315e-04 3.51172057e-03]\n",
      "  apples: 45.0%\n",
      "  pears: 35.1%\n",
      "  mangos: 7.2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images = find_objects(\"../test_images/apple.jpg\")\n",
    "\n",
    "# for i in range(len(images)):\n",
    "#     cv2.imshow(f\"{i}\", images[i])\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
